{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ARL SVM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNqAh4AEGrO6ynOo+Hurp8d"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7YRJ9c1rUqe3",
        "outputId": "0f678795-a497-48cc-d234-bfffbb983893"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import datetime\n",
        "import sys\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from imblearn import over_sampling\n",
        "from imblearn.over_sampling import SVMSMOTE\n",
        "\n",
        "def overSample(X,y):\n",
        "    \n",
        "    oversample = SVMSMOTE(n_jobs=-1,random_state=47)\n",
        "    X, y = oversample.fit_resample(X, y)\n",
        "    return X,y\n",
        "\n",
        "def run_model(vectorizer,pipe,param_grid):\n",
        "    content=''\n",
        "    \n",
        "    training_df = pd.read_csv('PreTraining.csv')\n",
        "    testing_df = pd.read_csv('PreTesting.csv')\n",
        "    #X_train, X_test, y_train, y_test = train_test_split(training_df['text2'], training_df['target'],\n",
        "    #X_train, X_test, y_train, y_test = train_test_split(training_df['processed'], training_df['target'],\n",
        "    #test_size=0.30, # should try different values\n",
        "    #random_state=42)\n",
        "            \n",
        "    #should balance based on target\n",
        "\n",
        "    X_train = training_df['processed']\n",
        "    y_train = training_df['target']\n",
        "    X_test = testing_df['processed']\n",
        "    y_test = testing_df['target']\n",
        "\n",
        "        #should balance the target\n",
        "\n",
        "    grid = GridSearchCV(\n",
        "        pipe,\n",
        "        param_grid,\n",
        "        cv=10,\n",
        "        scoring=['accuracy'],\n",
        "        refit='accuracy',\n",
        "        return_train_score=True,\n",
        "        n_jobs=32,\n",
        "        #error_score=1,\n",
        "        #verbose=4,\n",
        "        )\n",
        "    start_time = datetime.datetime.now()\n",
        "    grid.fit(X_train,y_train)\n",
        "    #grid.transform(X_train)\n",
        "    y_pred=grid.predict(X_test)\n",
        "    test_score=accuracy_score(y_test, y_pred)\n",
        "    finish_time = datetime.datetime.now()\n",
        "    content+=(\"\\n\"+vectorizer+\"\\nstart_time\\t\\t\\t\\t\"+\n",
        "        start_time.strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
        "    seconds = (finish_time - start_time).total_seconds()\n",
        "    content+=(\"\\nseconds\\t\\t\\t\\t\"+\"{:.2f}\".format(seconds) )\n",
        "    content+=(\"\\nMinutes\\t\\t\\t\\t\"+ \"{:.2f}\".format(seconds/60) )\n",
        "    for k,v in grid.best_params_.items():\n",
        "        content+=(\"\\nparam: \"+str(k)+\"\\t\\t\\t\\t\"+str(v))\n",
        "    content+=(\"\\ngrid.best_score_\\t\\t\\t\\t\"+\"{:.4f}\".format(grid.best_score_))\n",
        "    content+=(\"\\ngrid.refit_time_\\t\\t\\t\\t\"+\"{:.4f}\".format(grid.refit_time_))\n",
        "    content+=(\"\\ntest_score\\t\\t\\t\\t\"+\"{:.4f}\".format(test_score))\n",
        "    content+=(\"\\n\")\n",
        "    file1 = open(\"results.txt\", \"a\")  # append mode\n",
        "    file1.write(content)\n",
        "    file1.close()\n",
        "    return content\n",
        "###### function end\n",
        "\n",
        "'''\n",
        "time\n",
        "figure : loss\n",
        "confusion matrix\n",
        "accuracy\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntime\\nfigure : loss\\nconfusion matrix\\naccuracy\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnG3qMWkZxZ5"
      },
      "source": [
        "##### CountVectorizer\n",
        "cv_pipe= Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('svc', SVC()),\n",
        "    ])\n",
        "cv_param_grid= {\n",
        "    'vect__stop_words':['english',None],\n",
        "    'vect__max_df':[0.25,0.35,0.5,0.75,1],\n",
        "    'vect__min_df':[1,0.0000088], #float(1/112500) = 0.0000088; 112500 ~ words count\n",
        "    'vect__ngram_range':[(1,1),(1,2)],\n",
        "    'svc__kernel':['rbf','linear', 'poly', 'sigmoid'],\n",
        "    'svc__C':[0.5,1.00,1.5],\n",
        "    'svc__degree':[1,3],\n",
        "}\n",
        "#run_model(cv_pipe,cv_param_grid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkeCGCoVZ0C3"
      },
      "source": [
        "### HashingVectorizer\n",
        "hash_pipe= Pipeline([\n",
        "    ('vect', HashingVectorizer()),\n",
        "    ('svc', SVC()),\n",
        "    ])\n",
        "hash_param_grid= {\n",
        "    'vect__stop_words':['english',None],\n",
        "    'vect__ngram_range':[(1,1),(1,2)],\n",
        "    'vect__norm':['l1','l2'],\n",
        "    \n",
        "    'svc__kernel':['rbf', 'poly',],\n",
        "    #'svc__C':[0.5,1.00,1.5],\n",
        "    'svc__degree':[1,3],\n",
        "}\n",
        "#run_model('HashVectorizer',hash_pipe,hash_param_grid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQFIbCbcVfhT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19354b16-4552-4f68-a301-84d2ceb90b15"
      },
      "source": [
        "##### TFIDF Vectorizer\n",
        "tfidf_pipe= Pipeline([\n",
        "    ('vect', TfidfVectorizer()),\n",
        "    ('svc', SVC()),\n",
        "    ])\n",
        "tfidf_param_grid= {\n",
        "    #'vect__stop_words':['english',None],\n",
        "    'vect__max_df':[0.0625,0.125,0.25,0.35,0.5,0.75,1],\n",
        "    'vect__min_df':[1,0.00002,0.0000088], #float(1/112500) = 0.0000088; 112500 ~ words count\n",
        "    'vect__ngram_range':[(1,1),(1,2)],\n",
        "    'vect__norm':['l1','l2'],\n",
        "    'vect__use_idf':[True,False],\n",
        "    'vect__smooth_idf':[True,False],\n",
        "    'vect__sublinear_tf':[True,False],\n",
        "    'svc__kernel':['rbf','linear', 'poly', 'sigmoid'],\n",
        "    'svc__C':[0.5,1.00,1.5],\n",
        "    'svc__degree':[1,3],\n",
        "}\n",
        "\n",
        "tfidf_param_grid= {\n",
        "    #'vect__stop_words':['english',None],\n",
        "    'vect__max_df':[0.125],\n",
        "    #'vect__min_df':[1,0.0000088], #float(1/112500) = 0.0000088; 112500 ~ words count\n",
        "    #'vect__ngram_range':[(1,1),],\n",
        "    #'vect__norm':['l2'],\n",
        "    'vect__use_idf':[False],\n",
        "    #'vect__smooth_idf':[True,False],\n",
        "    #'vect__sublinear_tf':[True,False],\n",
        "    'svc__kernel':['rbf','linear', 'poly'],\n",
        "    'svc__C':[0.5,1.00],   \n",
        "    'svc__degree':[1,3], \n",
        "}\n",
        "\n",
        "print( run_model('TFIDF Vectorizer',tfidf_pipe,tfidf_param_grid) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TFIDF Vectorizer\n",
            "start_time\t\t\t\t10/22/2021, 01:30:29\n",
            "seconds\t\t\t\t679.81\n",
            "Minutes\t\t\t\t11.33\n",
            "param: svc__C\t\t\t\t0.5\n",
            "param: svc__degree\t\t\t\t1\n",
            "param: svc__kernel\t\t\t\tpoly\n",
            "param: vect__max_df\t\t\t\t0.125\n",
            "param: vect__use_idf\t\t\t\tFalse\n",
            "grid.best_score_\t\t\t\t0.8034\n",
            "grid.refit_time_\t\t\t\t3.9009\n",
            "test_score\t\t\t\t0.8214\n",
            "\n"
          ]
        }
      ]
    }
  ]
}